{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pdb\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "\n",
    "Data format: json {article:, options:[[]], answers:[]}, the blanks are marked with \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(folder, suffix=None):\n",
    "    \"\"\"loads data as nested dicts/lists\"\"\"\n",
    "    lst = []\n",
    "    for root, dirs, files in os.walk(folder, topdown=False):\n",
    "        for name in files:\n",
    "            if 'ipynb' in root:\n",
    "                continue # jupyter tmp file\n",
    "            if suffix is None or suffix in root:\n",
    "                name = os.path.join(root, name)\n",
    "                with open(name) as f:\n",
    "                    tmp = json.load(f)\n",
    "                    lst.append(tmp)\n",
    "                    if not tmp['options']:\n",
    "                        raise\n",
    "    print(folder, suffix, len(lst))\n",
    "    return lst\n",
    "\n",
    "train_lst = loadData('ELE', 'train') \n",
    "val_lst = loadData('ELE', 'dev')\n",
    "test_lst = loadData('ELE', 'test')\n",
    "cloth_lst = loadData('CLOTH')\n",
    "clean_lst = [] \n",
    "i = 0\n",
    "\"\"\" remove duplicates\"\"\"\n",
    "for idx, item in enumerate(cloth_lst):\n",
    "    dup = False\n",
    "    for j in train_lst+val_lst: # no test from cloth, as expected\n",
    "        if item['options'] == j['options']:\n",
    "            dup = True\n",
    "            break\n",
    "    if not dup:\n",
    "        clean_lst.append(item)\n",
    "\n",
    "train_lst = train_lst + clean_lst\n",
    "tmp = train_lst[0]\n",
    "print(\"%d from cloth\"%len(clean_lst))\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Preprocessing\n",
    " [cls] and [sep] is added automatically (for BERT, 101 and 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLANK_ID = 1035 # bert_uncased for \"_\"\n",
    "MASK_ID = 103 # for BERT\n",
    "MAX_LEN = 512\n",
    "SEP_TOKEN = 102\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def foo(x):\n",
    "    x = x.encode('ascii')\n",
    "    return int.from_bytes(x, byteorder='little')\n",
    "\n",
    "class ClozeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    the simplest data format: {article, options, answers}\n",
    "    article and answers zero padded, options -1 padded\n",
    "    options that contains multiple tokens are truncated\n",
    "     94 articles longer than 512, articles that are much too long are not discarded here, but will be truncated by my BERT model. The ignored options are filled with A.\n",
    "     5677 answers contains more than 1 BERT tokens, but only 2 of them cannot be disinguished using the initial token\n",
    "     for BERT, BLANK_ID should be changed into [MASK]\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.meta = []\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        # how many answers contain multiple bert tokens?\n",
    "        cnt = 0\n",
    "        cnt1 = 0\n",
    "        # how many cannot be distinguished by the initial token?\n",
    "        cnt2 = 0\n",
    "        for item in tqdm(data_list):\n",
    "            # article\n",
    "            article = item['article'].lower()\n",
    "            article = tokenizer.encode(article)\n",
    "            length = len(article)\n",
    "            article = torch.tensor(article)\n",
    "            n_blanks_before = sum(article==BLANK_ID)\n",
    "            if length > MAX_LEN:\n",
    "                cnt1 += 1\n",
    "                article = article[:MAX_LEN]\n",
    "                article[-1] = SEP_TOKEN\n",
    "            n_blanks = sum(article==BLANK_ID)\n",
    "            article = (article * (article!=BLANK_ID).long())+(MASK_ID*(article==BLANK_ID).long())\n",
    "            \n",
    "            # answers\n",
    "            answers = [foo(i) - foo('A') for i in item['answers']][:n_blanks]\n",
    "            answers = torch.tensor(answers)\n",
    "            \n",
    "            # options\n",
    "            options = [[tokenizer.encode(word)[1:-1] for word in line] for line in item['options']][:n_blanks]\n",
    "            for i, option in enumerate(options):\n",
    "                if answers.shape[0]> 0:\n",
    "                    if len(option[answers[i]])>1:\n",
    "                        cnt += 1\n",
    "                        if option[answers[i]] in option[0:answers[i]]+option[answers[i]+1:]:\n",
    "                            cnt2 += 1\n",
    "                options[i] = [item[0] for item in option]\n",
    "            # [0] is [CLS], [-1] is sep\n",
    "            options = torch.tensor(options)\n",
    "            self.data.append({\"article\":article, \"options\":options, \"answers\":answers})\n",
    "            self.meta.append({\"n_blanks_before\":n_blanks_before, \"n_blanks_truncated\":n_blanks, \"article_length\":length})\n",
    "            \n",
    "        print(\"%d answers contains multiple tokens\"%(cnt))\n",
    "        print(\"%d articles exceeds max length\"%(cnt1))\n",
    "        print(\"%d answers cannot be decided using the initial token\"%(cnt2))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "def collate_fn(data_list):\n",
    "    batch = {}\n",
    "    max_len = {}\n",
    "    for key in data_list[0]:\n",
    "        max_len[key] = 0\n",
    "        for item in data_list:\n",
    "            max_len[key] = max(max_len[key], item[key].shape[0])\n",
    "        lst = [item[key] for item in data_list]\n",
    "        padding_value = 0\n",
    "        if key == 'answers':\n",
    "            padding_value = -1\n",
    "        batch[key] = pad_sequence(lst, batch_first = True, padding_value = padding_value)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "train_set = ClozeDataset(train_lst)\n",
    "print(len(train_set))\n",
    "with open(\"train_set\", 'wb') as f:\n",
    "    pickle.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or Loading the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_set\", 'rb') as f:\n",
    "    train_set = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained('bert-large-uncased')\n",
    "\n",
    "    def forward(self, article, options, answers=None):\n",
    "        attention_mask = (article > 99)\n",
    "        result = self.bert(article, attention_mask = attention_mask, labels=article)\n",
    "        # we compute our custom loss, so there is no need to set the labels\n",
    "        _, logit = result[0], result[1]\n",
    "        \n",
    "        b, l, dim = logit.shape\n",
    "        blank_mask = article == MASK_ID\n",
    "        blank_mask = blank_mask.unsqueeze(-1).expand(*logit.shape)\n",
    "        logit = torch.masked_select(logit, blank_mask).view(-1, dim)\n",
    "        \n",
    "        options = options.view(-1)\n",
    "        mask = options>0\n",
    "        options = torch.masked_select(options.view(-1), mask).view(-1, 4)\n",
    "        # removes the padding options\n",
    "\n",
    "        if not answers is None:\n",
    "            answers = answers.view(-1)\n",
    "            answers = torch.masked_select(answers, answers>=0)\n",
    "            # removes the padding answers\n",
    "            index = answers.long().unsqueeze(1)\n",
    "            answer_token = torch.gather(options, 1, index).view(-1)\n",
    "            # shape: (n_blanks)\n",
    "            CE = nn.CrossEntropyLoss(reduction='none')\n",
    "            loss = CE(input = logit, target = answer_token)\n",
    "            return loss\n",
    "        \n",
    "        else:\n",
    "            option_score = torch.gather(logit, 1, options)\n",
    "            prediction = torch.argmax(option_score, dim = 1).view(-1)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_set\", 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "train_loader = DataLoader(train_set, batch_size = 24, shuffle = True, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model = model.cuda()\n",
    "model = nn.DataParallel(model)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    torch.save({'model_dict': model.module.state_dict(),\\\n",
    "           'optimizer_dict': optimizer.state_dict()},\n",
    "           \"./checkpoint_\"+str(epoch))\n",
    "    for i, data in enumerate(tqdm(train_loader)):\n",
    "        article, options, answers = data['article'].cuda(), data['options'].cuda(), data['answers'].cuda()\n",
    "        loss = model(article, options, answers)\n",
    "        loss = loss.mean()\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        if i % 10 is 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if i%100 is 0:\n",
    "            plt.plot(losses)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"losses\", 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_lst = loadData('ELE', 'dev')\n",
    "val_set = ClozeDataset(val_lst)\n",
    "val_loader = DataLoader(val_set, batch_size = 1, shuffle = False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0.\n",
    "total = 0.\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(val_loader):\n",
    "        article, options, answers = data['article'].cuda(), data['options'].cuda(), data['answers'].cuda()\n",
    "        pred = model.module(article, options)\n",
    "        answers = answers.view(-1)\n",
    "        answers = torch.masked_select(answers, answers>=0)\n",
    "        correct += (pred == answers).sum()\n",
    "        total += pred.shape[0]\n",
    "print(\"acc:\", correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lst = loadData('ELE','test')\n",
    "test_set = ClozeDataset(test_lst)\n",
    "test_loader = DataLoader(test_set, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "model.cuda()\n",
    "model.eval()\n",
    "results = {}\n",
    "for i, data in enumerate(tqdm(test_loader)):\n",
    "    result = []\n",
    "    article, options = data['article'], data['options']\n",
    "    article, options = article.cuda(), options.cuda()\n",
    "    prediction = model(article, options)\n",
    "    for j in range(test_set.meta[i]['n_blanks_before']):\n",
    "        if j < prediction.shape[0]:\n",
    "            result.append(chr(ord('A')+prediction[j]))\n",
    "        else:\n",
    "            result.append('A')\n",
    "    results[\"test%04d\"%i] = result\n",
    "    \n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_dict': model.module.state_dict(),\\\n",
    "           'optimizer_dict': optimizer.state_dict()},\n",
    "           \"./checkpoint_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./checkpoint_7\")\n",
    "model.module.load_state_dict(state_dict['model_dict'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
